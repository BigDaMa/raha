\section{Introduction}
\label{sec:intro}

Sensor glitches, submission of incorrect input, and malicious activities are a few examples of events that can lead to the appearance of outliers in a dataset. If undetected, these values can skew statistics, support invalid conclusions, slow database operations, and cause otherwise avoidable expenses. On the other hand, careful analysis of these values can yield new insight about the data, prevent undesirable events, and generally improve the reliability of the data~\cite{Achour2014}.

Previous literature has generally defined an outlier as ``an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism'' \cite{Hawkins1980}, and has suggested a number of ways to detect and in some cases eliminate suspicious values. Previous approaches to outlier detection include modeling numerical data using Gaussian Mixture Models~\cite{Lu2005,Roberts1994,Roberts1999}, Histogram modeling~\cite{Gebski2007,Sheng2007}, and $k$-nearest neighbors~\cite{Ramaswamy2000}.

Little work, however, has focused on developing generic methods for user-guided outlier detection that provide sufficient flexibility to offer insight on widely diverse and heterogeneous data stored in typical relational database management systems. The relative inexpressivity of basic SQL types -- integers, strings, and doubles in particular -- might be to blame: strings, for example, can be used to store information as diverse as city names, email addresses, or phone numbers; the task rests on application logic to parse these values in and out of the databases. Instead, this paucity of semantic information leaves outlier detection algorithms with very little information to work with.

This paper presents an innovative approach for detecting outliers in highly heterogeneous datasets: our tools systematically expand semantically poor SQL types to derive richer information.
This is done automatically, by identifying semantic expansions that are consistent with the bulk of the values that appear in each column.
This derived information can be used to detect outliers that are difficult or impossible to detect using raw data and provide detailed reports to the user. 
Our main contribution is a method that automatically applies a set of type-dependent rules, which map values of one type to a set of derived attributes.
%(all integers, for example, are expanded into dates and times by considering them as Unix timestamps, and into sets of booleans by considering them as bit vectors). 
Rules that best match the values appearing in the column are retained.
These derived attributes often show more structure than the original values, allowing data models to efficiently learn soft constraints about the data. 
%For example, if most of the values in a string column are in title case, then our models derive a soft constraint about the proper casing of values of that column, and report records with incorrectly formatted values. Similarly, if most of the dates reconstructed from an integer column fall on the same day of the week, then values falling on other days are flagged as suspicious. 
Expanded data can also be analyzed further to detect soft functional dependencies between derived attributes, enabling multidimensional models to detect of a broad class of data inconsistencies.

We designed the system to be both fast and memory efficient; it proceeds in three online passes over the data, keeping no more information than strictly necessary (in general, no more than a few dozen values per field in the database schema). The architecture is parallelizable, the analysis can be distributed over multiple computation nodes, and information can be kept from one run to the next so as to eliminate the first and possibly the second pass.

Our contributions are as follows:
\begin{enumerate}
\item We propose a method to provide semantically rich annotations, or \emph{expansions}, which automatically and efficiently extract and isolate information from field values.
%\item We discuss the idea of tuple expansion and a set of rules that prove useful in flagging outliers in heterogeneous datasets.
\item We build a system that detect outliers in both numerical and non-numerical, homo- and heterogeneous datasets.
%\item We propose a histogram-based approach to detect outliers in non-numerical, heterogeneous datasets.
\item We evaluate the performance of our system on several synthetic and real-world datasets.
\end{enumerate}

The rest of this paper is structured as follows: In Section~\ref{sec:expansion} we detail our tuple expansion method. In Section~\ref{sec:implementation} we apply tuple expansion to outlier detection and discuss the technical aspects of our tool. We evaluate our tool on synthetic and real-world datasets in Section~\ref{sec:evaluation}, and we describe related work in Section~\ref{sec:related-work}. Finally, we conclude in Section~\ref{sec:conclusion}. % TODO \item We outline directions for future work in Section~\ref{sec:future-work}.
