# Progress since last time

*   Pearson's correlation is implemented and working
*   Fitting mixture models is now faster, because it doesn't require scoring all
    samples anymore (not pushed yet)
*   We now have a command line interface for dboost-stdin and dboost-mimic2!
*   We have a new, discrete model based on histograms. This performs a lot better
    than the previous simplistic univariate Gaussian model.
*   We now support non-numerical features, including string signatures based on
    Unicode categories.

# Ideas

*   It's hard to derive which particular columns caused a sample to be classified
    as an outlier by the mixture-based model. Computing the direction of the gradient
    at the position corresponding to the outlier was suggested as a solution, but this,
    too, is hard because:

    1.  Computing small variations along a single axis is bound to return more
        outliers (this is due to multiple features being derived from a single
        field, and thus correlated).
    2.  The explicit expression of a mixture model is hard to compute (but it
        could be done, just feed the formula into any formal math package)
    3.  One could try to experimentally find a good candidate for the gradient,
        but that requires exploring a `nb_features`-dimensional sphere, with
        `nb_features` approximately equal to 30 in our examples, which is just
        impractical.
    4.  One could also try to compute deviations on the fields themselves,
        instead of the numerical features; but this is tricky, because it's not
        clear how to continuously alter a string. This would be feasible with
        (most of) our current features, but hackish and disagreeable.
    
    More generally, building mixture models is costly, and that's a problem. 

*   It's not obvious how to use correlation coefficients to find outliers. Looking
    at how each new tuple affects the correlation doesn't sound too promising.

# Interesting directions to explore

*   One could use correlations as a way to add new features. Now that we have
    extended the definition of features beyond simple numbers (we have strings,
    too, now), we could also include pairs of features. These pairs could be
    obtained based on suggestions from a correlation module; that module would
    find seemingly correlated pairs, and produce new features/rules based on
    these correlations; then, these rules would be used in computing the fully
    expanded tuples before passing them to the existing models. The models would
    need to be expanded to work on tuples (mainly, the definition of `plus`
    should be adjusted).

*   Finding a way to compute mixture model parameters online would be a neat
    research direction, assuming it hasn't been done.

*   There's a generally interesting problem in trying to find models for classes
    of things based just on positive samples. The 'signature' feature is a
    feeble attempt at this.

# Planning

Next meeting right after the DB exam on 2014-12-03. By then:

*   Cl√©ment documents the CLI.
*   Zelda fixes the mixture model to conform to the new interfaces and adds the
    new parameters to the CLI.
*   Rachael looks at using Pearson's correlation measures as a way to generate
    data-dependent rules.
*   Someone possibly does a literature search about online mixture models.
*   Everyone is welcome to hack on anything else :)
*   Study for DB exam!
