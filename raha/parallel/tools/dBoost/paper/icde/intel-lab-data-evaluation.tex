\subsection{Intel Lab Data}
\label{sec:intel-lab-data-evaluation}

We also evaluated our outlier detection framework on sensor data from the publicly available Intel Lab Data set\footnote{\url{http://db.csail.mit.edu/labdata/labdata.html}}. The Intel Lab Data contains data collected from 54 sensors spread throughout the Intel Berkeley Research Lab. Each data entry contains information including temperature, humidity, light and voltage taken from a Mica2Dot sensor and weatherboard. The dataset contains a total of approximately 2.3 million measurements.

The Intel lab dataset has known outliers from faulty sensor readings due to periods of critically low voltage. During these periods, the sensors go haywire and produce faulty measurements. For example, the temperature may be registered as over $120$ degrees Celsius, which is obviously abnormal behavior in a human environment such as where the sensors were deployed.
 
We analyzed a sample of 1000 data points selected at random from the sensor data; due to the numerical nature of this data, the Simple Gaussian and Mixture models are better-suited to analyzing it than the Histogram model.

We also compare the results of our models to Local Outlier Factors, a common outlier detection methodology, in this section.
 
\subsubsection{Simple Gaussian Model}

The results from running the sensor data set through the Simple Gaussian model are shown in Figure~\ref{fig:gauss15}. The data is plotted in light green, and the outliers are marked by dark red crosses. 

In this experiment we flag the entries with column values that fall outside $1.5$ standard deviations of the mean of that particular column as outliers. This model runs relatively fast, as no correlations are computed (\timing{.03}{0}{.12}).
%As we observe in Figure~\ref{fig:gauss15}, this leads to the extreme values on the temperature spectrum as well as the humidity spectrum to be identified as outliers.
%However, many points in the normal range of operation are flagged as outliers.
%The cause of the noise can be seen in Figure~\ref{fig:sensors_gaus_1-5b}.
%In this plot we see that the values of both voltage and light are relatively evenly distributed within their respective ranges.
%Therefore many values lie outside $1.5$ standard deviations from the mean, despite having reasonable values in other dimensions such as temperature.
%Using information from correlations is one way in which we improve on this, which we describe in the next section.


\subsubsection{Mixture Model}
We set the statistical threshold to $0.7$, which produces two correlations between temperature and humidity and between temperature and voltage. 
Figure~\ref{fig:gmm1t1} shows the results when using a single Gaussian component. Points flagged as outliers have a likelihood of less than $7.5\%$ of being produced by the Gaussian generated by the model (\timing {.03}{.34}{.73}).

This model is able to detect values with high temperature and low voltage as outliers.
 
%We show in Figure~\ref{fig:sensors_nocorr} the benefits of pruning the data via correlations before feeding it into the Gaussian model.
%In this experiment, we allow the statistical analyzer to mark all columns as correlated.
%The model produces a Gaussian with so much noise from the light data that it detects many points even within the normal operating range as outliers, even when the threshold is reduced to $0.5\%$. 
%Thus, using some mechanism to find correlations is useful in narrowing the search space for outliers.
 
Figure~\ref{fig:gmm2t05} shows the results obtained using the Mixture model with two components, using the same 1000 randomly selected data points. Flagged values have a likelihood of less than $7.5\%$ under their dominant Gaussian (\timing {.03}{.35}{0.78}).
When using two Gaussians, the points clustered around the temperature $120$ degrees Celsius are no longer detected as outliers: they are modeled by their own Gaussian (although this Gaussian's weight is smaller than its counterpart). This model highlights the points within normal sensor operation that have outlying results.


\subsubsection{Local Outlier Factors}
\label{sec:lof-evaluation}

In this section we compare the results of our Gaussian and Mixture models to Local Outlier Factors (LOF)~\cite{Breunig2000}, a frequently used method for outlier detection.
LOF measures the degree to which a data point is an outlier by comparing each data point's reachability to those of its $k$ nearest neighbors.
The higher the LOF, the more isolated the data point relative to its local neighborhood and therefore the more likely the point is to be an outlier.

One downside of LOF compared to \dBoost/ is that it can only evaluate two-dimensional data.
The original algorithm also has significant computation complexity in order to calculate the distance to the nearest neighbors of each data point.
One benefit of LOF, however, is that the algorithm returns a continuous value that indicates the degree to which a point is an outlier, as opposed to a binary value.

Figure~\ref{fig:lofk2} shows the outliers detected by LOF when $k=2$.
We observe that contrary to the Gaussian and Mixture models, the outliers detected by LOF are scattered throughout the data.
The outliers are not necessarily the points one would intuitively assume are outliers.
This is because points that are within the normal range of the data will be selected as outliers if they are far enough away from the other points nearest to them.
%When the number of nearest neighbors evaluated is increased to $10$ in Figure~\ref{fig:lof_10}, the outliers detected are further outside the main cluster of points.
We find that LOF is not as useful at pointing out the tagged outliers in the sensor data set.


