We thank their reviewers for their detailed suggestions and for their excellent feedback. The reviews highlight two main weaknesses:

* Useful extraction rules may be hard to create. While many general-purpose rules are widely applicable (string signature, for example, is useful for all kinds of semi-structured string identifiers, such as part numbers, booking references, tracking numbers, social security numbers, etc.), it is true that rules embedding domain-specific knowledge will in general perform better than generic rules (as a side note, since rules are simply Python functions annotated with types, we do not believe that writing rules presents technical challenges)
  Taken to the extreme, this is an instance of the following problem: if one wants to write fully accurate rules to detect outliers, one needs to know exactly what constraints the data obeys. The flexibility that extraction rules offer, however, is that even without perfect understanding of the data users can control the trade-off between accuracy of the outlier detection and amount of domain-specific knowledge fed into the system. In that sense, using extraction rules is a marked improvement over checking boolean properties on the data: significant insight can be extrapolated from little domain-specific knowledge.
  Another convenient aspect of tuple extraction is that the rules tend to be applicable to more than one particular dataset. For this reason, one can imagine building large collections of rules of limited applicability, shared among the users of our framework: at modeling time, if these rules do not separate outliers clearly, the expansions that they produce will not be taken into account. Such a collection of rules, gathered from users, would in itself be an interesting object of study, and may lead to the elaboration of such meta-rules as suggested by reviewer 1 (interestingly, a related notion is already present in the framework: higher-order functions are used to generate e.g. the rules that extract the various bits of an integer value).

* Performance may be an issue, especially on high-dimensional datasets.
  First, computing expansions with many rules may be costly. There are numerous ways to improve performance (as reviewer 2 points out, efficient data structures can save computation), and fortunately our system is amenable to many optimizations: indeed, since expansion rules are simply Python functions, rules can keep persistent state between invocations. It would thus be trivial, for example, to add caching (in fact, thanks to Python's decorator syntax, this could boil down to adding an `@memo` annotation to each rule). Rule-specific optimizations across tuples are also possible thanks to rule-private state.
  Second, even with optimized expansion rules, the sheer number of expanded tuple fields in high-dimensional datasets may be an issue. In this case, just as models can be trained on a relatively small sample of the data, relevant rules can be selected on a limited subset of the dataset. This already happens in two places: histogram generation (uninteresting histograms are discarded after processing only a small number of tuples, thus progressively reducing the set of columns for which histograms are built to just a few relevant columns), and correlation detection.

We hope that these responses address the main concerns of the reviewers, and we thank them again for the quality of their feedback. Brief responses to other comments are given below:

* The framework is implemented in Python 3.
* The current system contains about 20 generally-applicable rules, shown in [http://git.io/dBoost-rules].
* The Intel lab data experiment provides an example of outlier detection on a commonly studied real-life dataset: there, expansion rules allow simple models to perform as well as or better than Local Outlier Factor, a popular outlier detection method.
